{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb72575-622e-4540-9e67-eed9b4a26654",
     "showTitle": false,
     "tableResultSettingsMap": {
      "5": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761565810641}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 5
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "\n",
    "# Para visualizaciones\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "except:\n",
    "    print(\"⚠️ Matplotlib/Seaborn no disponibles. Instalando...\")\n",
    "    %pip install matplotlib seaborn\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 🎯 Clase de Configuración y Constantes\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuración centralizada\"\"\"\n",
    "    \n",
    "    # Tablas\n",
    "    class Tables:\n",
    "        MATERIALES = 'damm_silver_des.dm.dm_material_1'\n",
    "        VENTAS = 'damm_silver_des.gest_com_horeca.ca_crm_ventas_posicion'\n",
    "        RVL = 'damm_silver_des.gest_com_horeca.ca_crm_rentabilidad'\n",
    "        OUTPUT = 'damm_silver_des.pricing_unho.rentabilidad_consolidada'  \n",
    "    \n",
    "    # Parámetros de validación\n",
    "    FECHA_INICIO = \"2024-01-01\"\n",
    "    FECHA_FIN = \"2025-12-31\"\n",
    "    SECTOR_ESPERADO = '1'\n",
    "    MATERIALES_EXCLUIDOS = [\"TB8\", \"TB10\", \"TB12\"]\n",
    "    \n",
    "    # Umbrales de calidad\n",
    "    class Thresholds:\n",
    "        MAX_NULL_PERCENTAGE = 0.05  # 5% máximo de nulls permitido\n",
    "        MAX_DUPLICATE_PERCENTAGE = 0.01  # 1% máximo de duplicados\n",
    "        MAX_RECORD_LOSS_PERCENTAGE = 0.001  # 0.1% pérdida máxima en joins\n",
    "        MIN_RECORDS_EXPECTED = 1000  # Mínimo de registros esperados\n",
    "        OUTLIER_ZSCORE_THRESHOLD = 3  # Z-score para detección de outliers\n",
    "\n",
    "class ClasesCondiciones:\n",
    "    \"\"\"Clasificación de condiciones comerciales\"\"\"\n",
    "    Tarifa = [\"050\"]\n",
    "    Obsequios = [\"100\"]\n",
    "    Promociones = [\"300\"]\n",
    "    Descuentos = [\"210\"]\n",
    "    Contratos = [\"400\"]\n",
    "    AmortizacionesCDT = [\"420\"]\n",
    "    Rappels = [\"740\"]\n",
    "    CostesDistribuidor = [\"900\", \"901\", \"920\", \"902\", \"930\"]\n",
    "    Costes = [\"858\", \"859\", \"861\", \"890\", \"954\", \"956\"]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all_codes(cls) -> List[str]:\n",
    "        \"\"\"Retorna todos los códigos definidos\"\"\"\n",
    "        all_codes = []\n",
    "        for attr_name in dir(cls):\n",
    "            if not attr_name.startswith('_') and attr_name != 'get_all_codes':\n",
    "                attr_value = getattr(cls, attr_name)\n",
    "                if isinstance(attr_value, list):\n",
    "                    all_codes.extend(attr_value)\n",
    "        return all_codes\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 🛠️ Clase de Utilidades para Data Quality\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Clase para realizar chequeos de calidad de datos\"\"\"\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.results = []\n",
    "        self.test_count = 0\n",
    "        self.passed_count = 0\n",
    "        self.failed_count = 0\n",
    "    \n",
    "    def log_test(self, test_name: str, status: str, message: str, \n",
    "                 severity: str = \"ERROR\", details: Dict = None):\n",
    "        \"\"\"Registra resultado de un test\"\"\"\n",
    "        self.test_count += 1\n",
    "        if status == \"PASS\":\n",
    "            self.passed_count += 1\n",
    "            icon = \"✅\"\n",
    "        else:\n",
    "            self.failed_count += 1\n",
    "            icon = \"❌\"\n",
    "        \n",
    "        result = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"test_number\": self.test_count,\n",
    "            \"test_name\": test_name,\n",
    "            \"status\": status,\n",
    "            \"severity\": severity,\n",
    "            \"message\": message,\n",
    "            \"details\": details or {}\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        print(f\"{icon} Test #{self.test_count}: {test_name} - {status}\")\n",
    "        print(f\"   {message}\")\n",
    "        if details:\n",
    "            print(f\"   Details: {json.dumps(details, indent=2)}\")\n",
    "        print()\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Retorna resumen de tests\"\"\"\n",
    "        return {\n",
    "            \"total_tests\": self.test_count,\n",
    "            \"passed\": self.passed_count,\n",
    "            \"failed\": self.failed_count,\n",
    "            \"success_rate\": f\"{(self.passed_count/self.test_count*100):.2f}%\" if self.test_count > 0 else \"0%\"\n",
    "        }\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Muestra resumen visual\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        print(\"=\" * 80)\n",
    "        print(\"📊 DATA QUALITY TEST SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total Tests Run: {summary['total_tests']}\")\n",
    "        print(f\"✅ Passed: {summary['passed']}\")\n",
    "        print(f\"❌ Failed: {summary['failed']}\")\n",
    "        print(f\"Success Rate: {summary['success_rate']}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Crear DataFrame de resultados\n",
    "        if self.results:\n",
    "            df_results = spark.createDataFrame(self.results)\n",
    "            display(df_results)\n",
    "    \n",
    "    def check_null_percentage(self, df: DataFrame, column: str, \n",
    "                             threshold: float, test_name: str = None) -> bool:\n",
    "        \"\"\"Valida que el porcentaje de nulls no supere el umbral\"\"\"\n",
    "        if test_name is None:\n",
    "            test_name = f\"Null Check - {column}\"\n",
    "        \n",
    "        total_count = df.count()\n",
    "        null_count = df.filter(F.col(column).isNull()).count()\n",
    "        null_percentage = (null_count / total_count * 100) if total_count > 0 else 0\n",
    "        \n",
    "        passed = null_percentage <= (threshold * 100)\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        \n",
    "        message = f\"Column '{column}': {null_percentage:.2f}% nulls (threshold: {threshold*100}%)\"\n",
    "        details = {\n",
    "            \"column\": column,\n",
    "            \"total_records\": total_count,\n",
    "            \"null_count\": null_count,\n",
    "            \"null_percentage\": f\"{null_percentage:.2f}%\",\n",
    "            \"threshold\": f\"{threshold*100}%\"\n",
    "        }\n",
    "        \n",
    "        self.log_test(test_name, status, message, details=details)\n",
    "        return passed\n",
    "    \n",
    "    def check_duplicates(self, df: DataFrame, key_columns: List[str], \n",
    "                        threshold: float, test_name: str = None) -> bool:\n",
    "        \"\"\"Valida que no haya duplicados en las columnas clave\"\"\"\n",
    "        if test_name is None:\n",
    "            test_name = f\"Duplicate Check - {', '.join(key_columns)}\"\n",
    "        \n",
    "        total_count = df.count()\n",
    "        distinct_count = df.select(key_columns).distinct().count()\n",
    "        duplicate_count = total_count - distinct_count\n",
    "        duplicate_percentage = (duplicate_count / total_count * 100) if total_count > 0 else 0\n",
    "        \n",
    "        passed = duplicate_percentage <= (threshold * 100)\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        \n",
    "        message = f\"Keys {key_columns}: {duplicate_percentage:.2f}% duplicates (threshold: {threshold*100}%)\"\n",
    "        details = {\n",
    "            \"key_columns\": key_columns,\n",
    "            \"total_records\": total_count,\n",
    "            \"distinct_records\": distinct_count,\n",
    "            \"duplicates\": duplicate_count,\n",
    "            \"duplicate_percentage\": f\"{duplicate_percentage:.2f}%\",\n",
    "            \"threshold\": f\"{threshold*100}%\"\n",
    "        }\n",
    "        \n",
    "        self.log_test(test_name, status, message, details=details)\n",
    "        return passed\n",
    "    \n",
    "    def check_record_count(self, df: DataFrame, min_expected: int, \n",
    "                          test_name: str = None) -> bool:\n",
    "        \"\"\"Valida que el dataframe tenga al menos el número mínimo de registros\"\"\"\n",
    "        if test_name is None:\n",
    "            test_name = \"Minimum Record Count Check\"\n",
    "        \n",
    "        actual_count = df.count()\n",
    "        passed = actual_count >= min_expected\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        \n",
    "        message = f\"Record count: {actual_count:,} (minimum expected: {min_expected:,})\"\n",
    "        details = {\n",
    "            \"actual_count\": actual_count,\n",
    "            \"min_expected\": min_expected,\n",
    "            \"difference\": actual_count - min_expected\n",
    "        }\n",
    "        \n",
    "        self.log_test(test_name, status, message, details=details)\n",
    "        return passed\n",
    "    \n",
    "    def check_value_in_list(self, df: DataFrame, column: str, \n",
    "                           valid_values: List, test_name: str = None) -> bool:\n",
    "        \"\"\"Valida que todos los valores de una columna estén en una lista válida\"\"\"\n",
    "        if test_name is None:\n",
    "            test_name = f\"Valid Values Check - {column}\"\n",
    "        \n",
    "        invalid_values = (\n",
    "            df.filter(~F.col(column).isin(valid_values))\n",
    "            .select(column)\n",
    "            .distinct()\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "        passed = len(invalid_values) == 0\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        \n",
    "        invalid_list = [row[column] for row in invalid_values]\n",
    "        message = f\"Column '{column}': Found {len(invalid_values)} invalid values\"\n",
    "        details = {\n",
    "            \"column\": column,\n",
    "            \"valid_values\": valid_values,\n",
    "            \"invalid_values\": invalid_list[:10],  # Mostrar máximo 10\n",
    "            \"total_invalid\": len(invalid_values)\n",
    "        }\n",
    "        \n",
    "        self.log_test(test_name, status, message, details=details)\n",
    "        return passed\n",
    "    \n",
    "    def compare_row_counts(self, df1: DataFrame, df2: DataFrame, \n",
    "                          df1_name: str, df2_name: str, \n",
    "                          threshold: float, test_name: str = None) -> bool:\n",
    "        \"\"\"Compara conteos de registros entre dos dataframes\"\"\"\n",
    "        if test_name is None:\n",
    "            test_name = f\"Row Count Comparison: {df1_name} vs {df2_name}\"\n",
    "        \n",
    "        count1 = df1.count()\n",
    "        count2 = df2.count()\n",
    "        \n",
    "        if count1 > 0:\n",
    "            diff_percentage = abs(count1 - count2) / count1 * 100\n",
    "        else:\n",
    "            diff_percentage = 100 if count2 > 0 else 0\n",
    "        \n",
    "        passed = diff_percentage <= (threshold * 100)\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        \n",
    "        message = f\"{df1_name}: {count1:,} | {df2_name}: {count2:,} | Diff: {diff_percentage:.2f}%\"\n",
    "        details = {\n",
    "            df1_name: count1,\n",
    "            df2_name: count2,\n",
    "            \"difference\": count1 - count2,\n",
    "            \"diff_percentage\": f\"{diff_percentage:.2f}%\",\n",
    "            \"threshold\": f\"{threshold*100}%\"\n",
    "        }\n",
    "        \n",
    "        self.log_test(test_name, status, message, details=details)\n",
    "        return passed\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1️⃣ VALIDACIÓN DE FUENTES (Source Validation)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 1.1 Validar Tabla de Materiales\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando tabla de MATERIALES...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dq = DataQualityChecker(spark)\n",
    "\n",
    "# Cargar tabla de materiales SIN filtros\n",
    "df_materiales_raw = spark.table(Config.Tables.MATERIALES)\n",
    "\n",
    "# Test 5: Verificar que los materiales excluidos existen en la tabla\n",
    "materiales_excluidos_en_tabla = (\n",
    "    df_materiales_raw\n",
    "    .filter(F.col(\"Material\").isin(Config.MATERIALES_EXCLUIDOS))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Aplicar filtros como en el código original\n",
    "df_materiales_filtered = (\n",
    "    df_materiales_raw\n",
    "    .filter(F.col(\"Sector\") == Config.SECTOR_ESPERADO)\n",
    "    .filter(~F.col(\"Material\").isin(Config.MATERIALES_EXCLUIDOS))\n",
    "    .select(\"Material\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Test 6: Verificar que después del filtro quedan materiales\n",
    "materiales_count = df_materiales_filtered.count()\n",
    "dq.check_record_count(\n",
    "    df_materiales_filtered,\n",
    "    min_expected=Config.Thresholds.MIN_RECORDS_EXPECTED,\n",
    "    test_name=\"Materiales - Después de Filtros\"\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Materiales después de filtros: {materiales_count:,}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 1.2 Validar Tabla de Rentabilidad (RVL)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando tabla de RENTABILIDAD (RVL)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cargar RVL con filtro de fechas\n",
    "df_rvl_raw = (\n",
    "    spark.table(Config.Tables.RVL)\n",
    "    .filter(F.col(\"Fecha\").between(Config.FECHA_INICIO, Config.FECHA_FIN))\n",
    ")\n",
    "\n",
    "# Test 1: Verificar que hay datos en el rango de fechas\n",
    "dq.check_record_count(\n",
    "    df_rvl_raw,\n",
    "    min_expected=Config.Thresholds.MIN_RECORDS_EXPECTED,\n",
    "    test_name=\"RVL - Datos en Rango de Fechas\"\n",
    ")\n",
    "\n",
    "# Test 2: Verificar columnas clave sin nulls\n",
    "for col in [\"Detallista\", \"Material\", \"Fecha\", \"claseOperacion\"]:\n",
    "    dq.check_null_percentage(\n",
    "        df_rvl_raw,\n",
    "        column=col,\n",
    "        threshold=Config.Thresholds.MAX_NULL_PERCENTAGE,\n",
    "        test_name=f\"RVL - {col} Nulls\"\n",
    "    )\n",
    "\n",
    "# Test 3: Verificar rango de fechas\n",
    "from datetime import datetime\n",
    "\n",
    "fecha_min = df_rvl_raw.agg(F.min(\"Fecha\")).collect()[0][0]\n",
    "fecha_max = df_rvl_raw.agg(F.max(\"Fecha\")).collect()[0][0]\n",
    "\n",
    "# Convert config dates to datetime.date\n",
    "fecha_inicio_dt = datetime.strptime(Config.FECHA_INICIO, \"%Y-%m-%d\").date()\n",
    "fecha_fin_dt = datetime.strptime(Config.FECHA_FIN, \"%Y-%m-%d\").date()\n",
    "\n",
    "if fecha_min >= fecha_inicio_dt and fecha_max <= fecha_fin_dt:\n",
    "    dq.log_test(\n",
    "        \"RVL - Rango de Fechas Correcto\",\n",
    "        \"PASS\",\n",
    "        f\"Fechas en rango: {fecha_min} a {fecha_max}\",\n",
    "        details={\"fecha_min\": str(fecha_min), \"fecha_max\": str(fecha_max)}\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"RVL - Rango de Fechas Correcto\",\n",
    "        \"FAIL\",\n",
    "        f\"Fechas fuera de rango esperado: {fecha_min} a {fecha_max}\",\n",
    "        details={\n",
    "            \"fecha_min_actual\": str(fecha_min),\n",
    "            \"fecha_max_actual\": str(fecha_max),\n",
    "            \"fecha_inicio_esperada\": Config.FECHA_INICIO,\n",
    "            \"fecha_fin_esperada\": Config.FECHA_FIN\n",
    "        }\n",
    "    )\n",
    "# Test 4: Verificar que CtaResNiv2 contiene códigos conocidos\n",
    "codigos_conocidos = ClasesCondiciones.get_all_codes()\n",
    "codigos_no_mapeados = (\n",
    "    df_rvl_raw\n",
    "    .select(\"CtaResNiv2\")\n",
    "    .distinct()\n",
    "    .filter(~F.col(\"CtaResNiv2\").isin(codigos_conocidos))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "if len(codigos_no_mapeados) == 0:\n",
    "    dq.log_test(\n",
    "        \"RVL - Todos los CtaResNiv2 Mapeados\",\n",
    "        \"PASS\",\n",
    "        \"Todos los códigos están definidos en ClasesCondiciones\",\n",
    "        details={\"codigos_conocidos\": len(codigos_conocidos)}\n",
    "    )\n",
    "else:\n",
    "    codigos_list = [row[\"CtaResNiv2\"] for row in codigos_no_mapeados]\n",
    "    dq.log_test(\n",
    "        \"RVL - Todos los CtaResNiv2 Mapeados\",\n",
    "        \"FAIL\",\n",
    "        f\"Se encontraron {len(codigos_no_mapeados)} códigos sin mapear\",\n",
    "        severity=\"WARNING\",\n",
    "        details={\n",
    "            \"codigos_no_mapeados\": codigos_list[:20],  # Mostrar máximo 20\n",
    "            \"total_no_mapeados\": len(codigos_no_mapeados)\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"\\n📊 Registros RVL en rango: {df_rvl_raw.count():,}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 1.3 Validar Tabla de Ventas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando tabla de VENTAS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_ventas_raw = spark.table(Config.Tables.VENTAS)\n",
    "\n",
    "# Test 1: Tabla existe y tiene datos\n",
    "dq.check_record_count(\n",
    "    df_ventas_raw,\n",
    "    min_expected=Config.Thresholds.MIN_RECORDS_EXPECTED,\n",
    "    test_name=\"Ventas - Tabla Existe\"\n",
    ")\n",
    "\n",
    "# Test 2: Columnas clave sin nulls\n",
    "for col in [\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"]:\n",
    "    dq.check_null_percentage(\n",
    "        df_ventas_raw,\n",
    "        column=col,\n",
    "        threshold=Config.Thresholds.MAX_NULL_PERCENTAGE,\n",
    "        test_name=f\"Ventas - {col} Nulls\"\n",
    "    )\n",
    "\n",
    "# Test 3: VentaNeta y CantidadLitros no deben ser todos nulls\n",
    "venta_neta_nulls = df_ventas_raw.filter(F.col(\"VentaNeta\").isNull()).count()\n",
    "cantidad_litros_nulls = df_ventas_raw.filter(F.col(\"CantidadLitros\").isNull()).count()\n",
    "total_ventas = df_ventas_raw.count()\n",
    "\n",
    "if venta_neta_nulls < total_ventas:\n",
    "    dq.log_test(\n",
    "        \"Ventas - VentaNeta tiene valores\",\n",
    "        \"PASS\",\n",
    "        f\"VentaNeta: {venta_neta_nulls:,} nulls de {total_ventas:,}\",\n",
    "        details={\"nulls\": venta_neta_nulls, \"total\": total_ventas}\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Ventas - VentaNeta tiene valores\",\n",
    "        \"FAIL\",\n",
    "        \"VentaNeta está completamente nula\",\n",
    "        details={\"nulls\": venta_neta_nulls, \"total\": total_ventas}\n",
    "    )\n",
    "\n",
    "print(f\"\\n📊 Registros Ventas: {total_ventas:,}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2️⃣ VALIDACIÓN DE TRANSFORMACIONES\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.1 Validar Join: RVL + Materiales\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando JOIN: RVL + Materiales...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Contar registros ANTES del join\n",
    "count_rvl_antes = df_rvl_raw.count()\n",
    "\n",
    "# Realizar el join (inner join)\n",
    "df_rvl_con_materiales = df_rvl_raw.join(\n",
    "    df_materiales_filtered,\n",
    "    on=\"Material\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "count_rvl_despues = df_rvl_con_materiales.count()\n",
    "\n",
    "# Test 1: Verificar pérdida de registros en el join\n",
    "perdida_registros = count_rvl_antes - count_rvl_despues\n",
    "perdida_porcentaje = (perdida_registros / count_rvl_antes * 100) if count_rvl_antes > 0 else 0\n",
    "\n",
    "if perdida_porcentaje <= Config.Thresholds.MAX_RECORD_LOSS_PERCENTAGE * 100:\n",
    "    dq.log_test(\n",
    "        \"Join RVL+Materiales - Pérdida de Registros\",\n",
    "        \"PASS\",\n",
    "        f\"Pérdida: {perdida_registros:,} registros ({perdida_porcentaje:.4f}%)\",\n",
    "        details={\n",
    "            \"antes_join\": count_rvl_antes,\n",
    "            \"despues_join\": count_rvl_despues,\n",
    "            \"perdida_registros\": perdida_registros,\n",
    "            \"perdida_porcentaje\": f\"{perdida_porcentaje:.4f}%\"\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Join RVL+Materiales - Pérdida de Registros\",\n",
    "        \"FAIL\",\n",
    "        f\"Pérdida significativa: {perdida_registros:,} registros ({perdida_porcentaje:.2f}%)\",\n",
    "        details={\n",
    "            \"antes_join\": count_rvl_antes,\n",
    "            \"despues_join\": count_rvl_despues,\n",
    "            \"perdida_registros\": perdida_registros,\n",
    "            \"perdida_porcentaje\": f\"{perdida_porcentaje:.2f}%\",\n",
    "            \"threshold\": f\"{Config.Thresholds.MAX_RECORD_LOSS_PERCENTAGE*100}%\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Test 2: Verificar que no hay materiales fuera del filtro\n",
    "materiales_invalidos = (\n",
    "    df_rvl_con_materiales\n",
    "    .join(df_materiales_filtered, on=\"Material\", how=\"left_anti\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "if materiales_invalidos == 0:\n",
    "    dq.log_test(\n",
    "        \"Join RVL+Materiales - Sin Materiales Inválidos\",\n",
    "        \"PASS\",\n",
    "        \"Todos los materiales son válidos después del join\"\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Join RVL+Materiales - Sin Materiales Inválidos\",\n",
    "        \"FAIL\",\n",
    "        f\"Se encontraron {materiales_invalidos} registros con materiales inválidos\",\n",
    "        details={\"materiales_invalidos\": materiales_invalidos}\n",
    "    )\n",
    "\n",
    "print(f\"\\n📊 Registros RVL antes del join: {count_rvl_antes:,}\")\n",
    "print(f\"📊 Registros RVL después del join: {count_rvl_despues:,}\")\n",
    "print(f\"📊 Pérdida: {perdida_registros:,} ({perdida_porcentaje:.4f}%)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.2 Validar Creación de AgregadorCondicion\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando creación de AgregadorCondicion...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def create_aggregator_column(col_name):\n",
    "    \"\"\"Replica la función del código original\"\"\"\n",
    "    condition = None\n",
    "    for categoria, codigos in ClasesCondiciones.__dict__.items():\n",
    "        if not categoria.startswith(\"__\") and not categoria.startswith(\"get_\"):\n",
    "            if condition is None:\n",
    "                condition = F.when(F.col(col_name).isin(codigos), F.lit(categoria))\n",
    "            else:\n",
    "                condition = condition.when(F.col(col_name).isin(codigos), F.lit(categoria))\n",
    "    return condition.otherwise(F.lit(None))\n",
    "\n",
    "df_rvl_con_agregador = df_rvl_con_materiales.withColumn(\n",
    "    \"AgregadorCondicion\",\n",
    "    create_aggregator_column(\"CtaResNiv2\")\n",
    ")\n",
    "\n",
    "# Test 1: Verificar que AgregadorCondicion no es null para códigos conocidos\n",
    "codigos_conocidos = ClasesCondiciones.get_all_codes()\n",
    "registros_con_codigo_conocido = df_rvl_con_agregador.filter(\n",
    "    F.col(\"CtaResNiv2\").isin(codigos_conocidos)\n",
    ").count()\n",
    "\n",
    "registros_con_agregador_null = df_rvl_con_agregador.filter(\n",
    "    F.col(\"CtaResNiv2\").isin(codigos_conocidos) & F.col(\"AgregadorCondicion\").isNull()\n",
    ").count()\n",
    "\n",
    "if registros_con_agregador_null == 0:\n",
    "    dq.log_test(\n",
    "        \"AgregadorCondicion - Sin Nulls para Códigos Conocidos\",\n",
    "        \"PASS\",\n",
    "        f\"Todos los {registros_con_codigo_conocido:,} registros con código conocido tienen AgregadorCondicion\"\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"AgregadorCondicion - Sin Nulls para Códigos Conocidos\",\n",
    "        \"FAIL\",\n",
    "        f\"{registros_con_agregador_null:,} registros con código conocido tienen AgregadorCondicion=null\",\n",
    "        details={\n",
    "            \"registros_con_codigo_conocido\": registros_con_codigo_conocido,\n",
    "            \"registros_null\": registros_con_agregador_null\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Test 2: Verificar distribución de AgregadorCondicion\n",
    "distribucion = (\n",
    "    df_rvl_con_agregador\n",
    "    .groupBy(\"AgregadorCondicion\")\n",
    "    .agg(F.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Distribución de AgregadorCondicion:\")\n",
    "display(distribucion)\n",
    "\n",
    "# Test 3: Verificar que cada código se mapea a una única categoría\n",
    "codigos_con_multiple_categoria = []\n",
    "for categoria, codigos in ClasesCondiciones.__dict__.items():\n",
    "    if not categoria.startswith(\"__\") and not categoria.startswith(\"get_\"):\n",
    "        for codigo in codigos:\n",
    "            # Verificar si el código está en otras categorías\n",
    "            apariciones = 0\n",
    "            for cat2, codes2 in ClasesCondiciones.__dict__.items():\n",
    "                if not cat2.startswith(\"__\") and not cat2.startswith(\"get_\"):\n",
    "                    if codigo in codes2:\n",
    "                        apariciones += 1\n",
    "            if apariciones > 1:\n",
    "                codigos_con_multiple_categoria.append(codigo)\n",
    "\n",
    "if len(codigos_con_multiple_categoria) == 0:\n",
    "    dq.log_test(\n",
    "        \"AgregadorCondicion - Códigos Únicos por Categoría\",\n",
    "        \"PASS\",\n",
    "        \"Cada código pertenece a una única categoría\"\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"AgregadorCondicion - Códigos Únicos por Categoría\",\n",
    "        \"FAIL\",\n",
    "        f\"Códigos duplicados en múltiples categorías: {codigos_con_multiple_categoria}\",\n",
    "        details={\"codigos_duplicados\": codigos_con_multiple_categoria}\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.3 Validar Agregación de RVL\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando agregación de RVL...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Agregar como en el código original\n",
    "df_rvl_agregado = (\n",
    "    df_rvl_con_agregador\n",
    "    .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"AgregadorCondicion\", \"claseOperacion\")\n",
    "    .agg(F.sum(\"MargenDetallado\").alias(\"MargenTotal\"))\n",
    "    .withColumnRenamed(\"claseOperacion\", \"ClaseOperacionID\")\n",
    ")\n",
    "\n",
    "# Test 1: Verificar que no se perdieron registros en la agregación\n",
    "count_antes_agg = df_rvl_con_agregador.count()\n",
    "count_despues_agg = df_rvl_agregado.count()\n",
    "\n",
    "dq.log_test(\n",
    "    \"Agregación RVL - Registros Después de Group By\",\n",
    "    \"PASS\" if count_despues_agg > 0 else \"FAIL\",\n",
    "    f\"Antes: {count_antes_agg:,} | Después: {count_despues_agg:,}\",\n",
    "    details={\n",
    "        \"antes_agregacion\": count_antes_agg,\n",
    "        \"despues_agregacion\": count_despues_agg,\n",
    "        \"reduccion\": count_antes_agg - count_despues_agg\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test 2: Verificar que MargenTotal no es null\n",
    "dq.check_null_percentage(\n",
    "    df_rvl_agregado,\n",
    "    column=\"MargenTotal\",\n",
    "    threshold=0.0,\n",
    "    test_name=\"Agregación RVL - MargenTotal No Null\"\n",
    ")\n",
    "\n",
    "# Test 3: Verificar reconciliación de suma (suma antes == suma después)\n",
    "suma_antes = df_rvl_con_agregador.agg(F.sum(\"MargenDetallado\")).collect()[0][0] or 0\n",
    "suma_despues = df_rvl_agregado.agg(F.sum(\"MargenTotal\")).collect()[0][0] or 0\n",
    "diferencia = abs(suma_antes - suma_despues)\n",
    "tolerancia = 0.01  # Tolerancia de 1 centavo por redondeos\n",
    "\n",
    "if diferencia <= tolerancia:\n",
    "    dq.log_test(\n",
    "        \"Agregación RVL - Reconciliación de Suma\",\n",
    "        \"PASS\",\n",
    "        f\"Suma antes: {suma_antes:,.2f} | Suma después: {suma_despues:,.2f} | Diff: {diferencia:.2f}\",\n",
    "        details={\n",
    "            \"suma_antes\": suma_antes,\n",
    "            \"suma_despues\": suma_despues,\n",
    "            \"diferencia\": diferencia,\n",
    "            \"tolerancia\": tolerancia\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Agregación RVL - Reconciliación de Suma\",\n",
    "        \"FAIL\",\n",
    "        f\"Diferencia en suma: {diferencia:,.2f} (tolerancia: {tolerancia})\",\n",
    "        details={\n",
    "            \"suma_antes\": suma_antes,\n",
    "            \"suma_despues\": suma_despues,\n",
    "            \"diferencia\": diferencia,\n",
    "            \"tolerancia\": tolerancia\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Test 4: Verificar que no hay duplicados en las claves\n",
    "dq.check_duplicates(\n",
    "    df_rvl_agregado,\n",
    "    key_columns=[\"Detallista\", \"Material\", \"Fecha\", \"AgregadorCondicion\", \"ClaseOperacionID\"],\n",
    "    threshold=0.0,\n",
    "    test_name=\"Agregación RVL - Sin Duplicados en Claves\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.4 Validar Ajuste de CostesDistribuidor\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando ajuste de CostesDistribuidor...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular MargenTarifa con window function (como en código original)\n",
    "windowSpec = Window.partitionBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "\n",
    "df_rvl_con_margen_tarifa = (\n",
    "    df_rvl_agregado\n",
    "    .withColumn(\n",
    "        \"MargenTarifa\",\n",
    "        F.sum(\n",
    "            F.when(F.col(\"AgregadorCondicion\") == \"Tarifa\", F.col(\"MargenTotal\"))\n",
    "            .otherwise(0)\n",
    "        ).over(windowSpec)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test 1: Verificar que todas las particiones con CostesDistribuidor tienen MargenTarifa\n",
    "costes_dist_sin_tarifa = (\n",
    "    df_rvl_con_margen_tarifa\n",
    "    .filter(F.col(\"AgregadorCondicion\") == \"CostesDistribuidor\")\n",
    "    .filter((F.col(\"MargenTarifa\").isNull()) | (F.col(\"MargenTarifa\") == 0))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "total_costes_dist = (\n",
    "    df_rvl_con_margen_tarifa\n",
    "    .filter(F.col(\"AgregadorCondicion\") == \"CostesDistribuidor\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "if costes_dist_sin_tarifa == 0:\n",
    "    dq.log_test(\n",
    "        \"Ajuste CostesDistribuidor - Todos tienen MargenTarifa\",\n",
    "        \"PASS\",\n",
    "        f\"Todos los {total_costes_dist:,} registros CostesDistribuidor tienen MargenTarifa\",\n",
    "        details={\"total_costes_distribuidor\": total_costes_dist}\n",
    "    )\n",
    "else:\n",
    "    porcentaje_sin_tarifa = (costes_dist_sin_tarifa / total_costes_dist * 100) if total_costes_dist > 0 else 0\n",
    "    dq.log_test(\n",
    "        \"Ajuste CostesDistribuidor - Todos tienen MargenTarifa\",\n",
    "        \"FAIL\",\n",
    "        f\"{costes_dist_sin_tarifa:,} CostesDistribuidor sin MargenTarifa ({porcentaje_sin_tarifa:.2f}%)\",\n",
    "        severity=\"WARNING\",\n",
    "        details={\n",
    "            \"total_costes_distribuidor\": total_costes_dist,\n",
    "            \"sin_margen_tarifa\": costes_dist_sin_tarifa,\n",
    "            \"porcentaje\": f\"{porcentaje_sin_tarifa:.2f}%\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Aplicar el ajuste\n",
    "df_rvl_ajustado = (\n",
    "    df_rvl_con_margen_tarifa\n",
    "    .withColumn(\n",
    "        \"MargenTotal_Original\",\n",
    "        F.col(\"MargenTotal\")  # Guardar original para comparación\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"MargenTotal\",\n",
    "        F.when(\n",
    "            F.col(\"AgregadorCondicion\") == \"CostesDistribuidor\",\n",
    "            F.col(\"MargenTotal\") - F.col(\"MargenTarifa\")\n",
    "        ).otherwise(F.col(\"MargenTotal\"))\n",
    "    )\n",
    "    .drop(\"MargenTarifa\")\n",
    ")\n",
    "\n",
    "# Test 2: Verificar que el ajuste se aplicó correctamente\n",
    "registros_ajustados = (\n",
    "    df_rvl_ajustado\n",
    "    .filter(F.col(\"AgregadorCondicion\") == \"CostesDistribuidor\")\n",
    "    .filter(F.col(\"MargenTotal\") != F.col(\"MargenTotal_Original\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "if registros_ajustados > 0:\n",
    "    dq.log_test(\n",
    "        \"Ajuste CostesDistribuidor - Aplicado Correctamente\",\n",
    "        \"PASS\",\n",
    "        f\"{registros_ajustados:,} registros ajustados\",\n",
    "        details={\n",
    "            \"registros_ajustados\": registros_ajustados,\n",
    "            \"total_costes_distribuidor\": total_costes_dist\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Ajuste CostesDistribuidor - Aplicado Correctamente\",\n",
    "        \"FAIL\",\n",
    "        \"No se aplicó ningún ajuste a CostesDistribuidor\",\n",
    "        severity=\"WARNING\"\n",
    "    )\n",
    "\n",
    "# Test 3: Reconciliación de suma después del ajuste\n",
    "suma_antes_ajuste = df_rvl_agregado.agg(F.sum(\"MargenTotal\")).collect()[0][0] or 0\n",
    "suma_despues_ajuste = df_rvl_ajustado.agg(F.sum(\"MargenTotal\")).collect()[0][0] or 0\n",
    "\n",
    "dq.log_test(\n",
    "    \"Ajuste CostesDistribuidor - Suma Cambió\",\n",
    "    \"PASS\",\n",
    "    f\"Suma antes: {suma_antes_ajuste:,.2f} | Suma después: {suma_despues_ajuste:,.2f}\",\n",
    "    details={\n",
    "        \"suma_antes\": suma_antes_ajuste,\n",
    "        \"suma_despues\": suma_despues_ajuste,\n",
    "        \"diferencia\": suma_antes_ajuste - suma_despues_ajuste\n",
    "    }\n",
    ")\n",
    "\n",
    "# Limpiar columna temporal\n",
    "df_rvl_ajustado = df_rvl_ajustado.drop(\"MargenTotal_Original\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.5 Validar Agregación de Ventas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando agregación de Ventas...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_ventas_agregadas = (\n",
    "    spark.table(Config.Tables.VENTAS)\n",
    "    .join(F.broadcast(df_materiales_filtered), on=\"Material\", how=\"inner\")\n",
    "    .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "    .agg(\n",
    "        F.sum(\"VentaNeta\").alias(\"VentaNeta\"),\n",
    "        F.sum(\"CantidadLitros\").alias(\"CantidadLitros\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test 1: Verificar que hay datos agregados\n",
    "dq.check_record_count(\n",
    "    df_ventas_agregadas,\n",
    "    min_expected=Config.Thresholds.MIN_RECORDS_EXPECTED,\n",
    "    test_name=\"Ventas Agregadas - Tiene Datos\"\n",
    ")\n",
    "\n",
    "# Test 2: Verificar sin duplicados en claves\n",
    "dq.check_duplicates(\n",
    "    df_ventas_agregadas,\n",
    "    key_columns=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"],\n",
    "    threshold=0.0,\n",
    "    test_name=\"Ventas Agregadas - Sin Duplicados\"\n",
    ")\n",
    "\n",
    "# Test 3: Reconciliación de suma de VentaNeta\n",
    "suma_ventas_raw = (\n",
    "    spark.table(Config.Tables.VENTAS)\n",
    "    .join(F.broadcast(df_materiales_filtered), on=\"Material\", how=\"inner\")\n",
    "    .agg(F.sum(\"VentaNeta\"))\n",
    "    .collect()[0][0] or 0\n",
    ")\n",
    "\n",
    "suma_ventas_agregadas = df_ventas_agregadas.agg(F.sum(\"VentaNeta\")).collect()[0][0] or 0\n",
    "diferencia = abs(suma_ventas_raw - suma_ventas_agregadas)\n",
    "\n",
    "if diferencia <= 0.01:\n",
    "    dq.log_test(\n",
    "        \"Ventas Agregadas - Reconciliación VentaNeta\",\n",
    "        \"PASS\",\n",
    "        f\"Raw: {suma_ventas_raw:,.2f} | Agregado: {suma_ventas_agregadas:,.2f}\",\n",
    "        details={\n",
    "            \"suma_raw\": suma_ventas_raw,\n",
    "            \"suma_agregada\": suma_ventas_agregadas,\n",
    "            \"diferencia\": diferencia\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Ventas Agregadas - Reconciliación VentaNeta\",\n",
    "        \"FAIL\",\n",
    "        f\"Diferencia: {diferencia:,.2f}\",\n",
    "        details={\n",
    "            \"suma_raw\": suma_ventas_raw,\n",
    "            \"suma_agregada\": suma_ventas_agregadas,\n",
    "            \"diferencia\": diferencia\n",
    "        }\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.6 Validar Pivot de RVL\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando pivot de RVL...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_rvl_pivotado = (\n",
    "    df_rvl_ajustado\n",
    "    .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "    .pivot(\"AgregadorCondicion\")\n",
    "    .agg(F.sum(\"MargenTotal\"))\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Test 1: Verificar que no se perdieron registros en el pivot\n",
    "count_antes_pivot = df_rvl_ajustado.select(\n",
    "    \"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"\n",
    ").distinct().count()\n",
    "\n",
    "count_despues_pivot = df_rvl_pivotado.count()\n",
    "\n",
    "dq.compare_row_counts(\n",
    "    df1=df_rvl_ajustado.select(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\").distinct(),\n",
    "    df2=df_rvl_pivotado,\n",
    "    df1_name=\"Antes Pivot (Unique Keys)\",\n",
    "    df2_name=\"Después Pivot\",\n",
    "    threshold=0.0,\n",
    "    test_name=\"Pivot RVL - Sin Pérdida de Registros\"\n",
    ")\n",
    "\n",
    "# Test 2: Verificar que existen las columnas esperadas del pivot\n",
    "columnas_esperadas = [\n",
    "    \"Tarifa\", \"Obsequios\", \"Promociones\", \"Descuentos\", \"Contratos\",\n",
    "    \"AmortizacionesCDT\", \"Rappels\", \"CostesDistribuidor\", \"Costes\"\n",
    "]\n",
    "\n",
    "columnas_pivotadas = [col for col in df_rvl_pivotado.columns if col not in [\n",
    "    \"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"\n",
    "]]\n",
    "\n",
    "columnas_faltantes = set(columnas_esperadas) - set(columnas_pivotadas)\n",
    "\n",
    "if len(columnas_faltantes) == 0:\n",
    "    dq.log_test(\n",
    "        \"Pivot RVL - Columnas Esperadas Creadas\",\n",
    "        \"PASS\",\n",
    "        f\"Todas las {len(columnas_esperadas)} columnas esperadas fueron creadas\",\n",
    "        details={\"columnas_pivotadas\": columnas_pivotadas}\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Pivot RVL - Columnas Esperadas Creadas\",\n",
    "        \"FAIL\",\n",
    "        f\"Faltan columnas: {list(columnas_faltantes)}\",\n",
    "        severity=\"WARNING\",\n",
    "        details={\n",
    "            \"columnas_esperadas\": columnas_esperadas,\n",
    "            \"columnas_creadas\": columnas_pivotadas,\n",
    "            \"columnas_faltantes\": list(columnas_faltantes)\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Test 3: Verificar que fillna(0) se aplicó (no hay nulls)\n",
    "null_counts = {}\n",
    "for col in columnas_pivotadas:\n",
    "    null_count = df_rvl_pivotado.filter(F.col(col).isNull()).count()\n",
    "    null_counts[col] = null_count\n",
    "\n",
    "total_nulls = sum(null_counts.values())\n",
    "\n",
    "if total_nulls == 0:\n",
    "    dq.log_test(\n",
    "        \"Pivot RVL - fillna(0) Aplicado\",\n",
    "        \"PASS\",\n",
    "        \"No hay nulls en las columnas pivotadas\"\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Pivot RVL - fillna(0) Aplicado\",\n",
    "        \"FAIL\",\n",
    "        f\"Se encontraron {total_nulls} nulls en columnas pivotadas\",\n",
    "        details={\"null_counts_por_columna\": null_counts}\n",
    "    )\n",
    "\n",
    "# Test 4: Reconciliación de suma después del pivot\n",
    "suma_antes_pivot = df_rvl_ajustado.agg(F.sum(\"MargenTotal\")).collect()[0][0] or 0\n",
    "\n",
    "# Sumar todas las columnas pivotadas\n",
    "suma_expresion = sum([F.coalesce(F.col(col), F.lit(0)) for col in columnas_pivotadas])\n",
    "suma_despues_pivot = df_rvl_pivotado.agg(F.sum(suma_expresion)).collect()[0][0] or 0\n",
    "\n",
    "diferencia = abs(suma_antes_pivot - suma_despues_pivot)\n",
    "\n",
    "if diferencia <= 0.01:\n",
    "    dq.log_test(\n",
    "        \"Pivot RVL - Reconciliación de Suma\",\n",
    "        \"PASS\",\n",
    "        f\"Antes: {suma_antes_pivot:,.2f} | Después: {suma_despues_pivot:,.2f}\",\n",
    "        details={\n",
    "            \"suma_antes\": suma_antes_pivot,\n",
    "            \"suma_despues\": suma_despues_pivot,\n",
    "            \"diferencia\": diferencia\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Pivot RVL - Reconciliación de Suma\",\n",
    "        \"FAIL\",\n",
    "        f\"Diferencia en suma: {diferencia:,.2f}\",\n",
    "        details={\n",
    "            \"suma_antes\": suma_antes_pivot,\n",
    "            \"suma_despues\": suma_despues_pivot,\n",
    "            \"diferencia\": diferencia\n",
    "        }\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2.7 Validar Join Final: RVL Pivotado + Ventas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando join final: RVL Pivotado + Ventas...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_final = df_rvl_pivotado.join(\n",
    "    df_ventas_agregadas,\n",
    "    on=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Test 1: Verificar que no se perdieron registros de RVL en el join\n",
    "dq.compare_row_counts(\n",
    "    df1=df_rvl_pivotado,\n",
    "    df2=df_final,\n",
    "    df1_name=\"RVL Pivotado\",\n",
    "    df2_name=\"Final (después de join)\",\n",
    "    threshold=0.0,\n",
    "    test_name=\"Join Final - Sin Pérdida de Registros RVL\"\n",
    ")\n",
    "\n",
    "# Test 2: Verificar que hay registros con ventas\n",
    "registros_con_ventas = df_final.filter(F.col(\"VentaNeta\").isNotNull()).count()\n",
    "total_registros = df_final.count()\n",
    "porcentaje_con_ventas = (registros_con_ventas / total_registros * 100) if total_registros > 0 else 0\n",
    "\n",
    "dq.log_test(\n",
    "    \"Join Final - Registros con Ventas\",\n",
    "    \"PASS\" if porcentaje_con_ventas > 50 else \"FAIL\",\n",
    "    f\"{registros_con_ventas:,} de {total_registros:,} tienen ventas ({porcentaje_con_ventas:.2f}%)\",\n",
    "    severity=\"WARNING\" if porcentaje_con_ventas <= 50 else \"INFO\",\n",
    "    details={\n",
    "        \"con_ventas\": registros_con_ventas,\n",
    "        \"total\": total_registros,\n",
    "        \"porcentaje\": f\"{porcentaje_con_ventas:.2f}%\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test 3: Verificar que VentaNeta y CantidadLitros son consistentes\n",
    "# Si hay VentaNeta, debería haber CantidadLitros\n",
    "registros_venta_sin_litros = df_final.filter(\n",
    "    (F.col(\"VentaNeta\").isNotNull()) & (F.col(\"CantidadLitros\").isNull())\n",
    ").count()\n",
    "\n",
    "if registros_venta_sin_litros == 0:\n",
    "    dq.log_test(\n",
    "        \"Join Final - Consistencia VentaNeta-CantidadLitros\",\n",
    "        \"PASS\",\n",
    "        \"Todos los registros con VentaNeta tienen CantidadLitros\"\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Join Final - Consistencia VentaNeta-CantidadLitros\",\n",
    "        \"FAIL\",\n",
    "        f\"{registros_venta_sin_litros:,} registros con VentaNeta pero sin CantidadLitros\",\n",
    "        severity=\"WARNING\",\n",
    "        details={\"registros_inconsistentes\": registros_venta_sin_litros}\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3️⃣ VALIDACIONES DE REGLAS DE NEGOCIO\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.1 Validar Rangos de Valores\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando rangos de valores...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: Verificar que no hay fechas fuera del rango esperado\n",
    "fechas_fuera_rango = df_final.filter(\n",
    "    (F.col(\"Fecha\") < Config.FECHA_INICIO) | (F.col(\"Fecha\") > Config.FECHA_FIN)\n",
    ").count()\n",
    "\n",
    "if fechas_fuera_rango == 0:\n",
    "    dq.log_test(\n",
    "        \"Reglas Negocio - Fechas en Rango\",\n",
    "        \"PASS\",\n",
    "        f\"Todas las fechas están entre {Config.FECHA_INICIO} y {Config.FECHA_FIN}\"\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Reglas Negocio - Fechas en Rango\",\n",
    "        \"FAIL\",\n",
    "        f\"{fechas_fuera_rango:,} registros con fechas fuera de rango\",\n",
    "        details={\"registros_fuera_rango\": fechas_fuera_rango}\n",
    "    )\n",
    "\n",
    "# Test 2: Verificar valores negativos en VentaNeta (puede ser válido, pero verificar)\n",
    "ventas_negativas = df_final.filter(F.col(\"VentaNeta\") < 0).count()\n",
    "total_con_ventas = df_final.filter(F.col(\"VentaNeta\").isNotNull()).count()\n",
    "porcentaje_negativas = (ventas_negativas / total_con_ventas * 100) if total_con_ventas > 0 else 0\n",
    "\n",
    "if porcentaje_negativas < 5:  # Menos del 5% es aceptable (devoluciones)\n",
    "    dq.log_test(\n",
    "        \"Reglas Negocio - VentaNeta Negativas\",\n",
    "        \"PASS\",\n",
    "        f\"{ventas_negativas:,} ventas negativas ({porcentaje_negativas:.2f}%) - Aceptable para devoluciones\",\n",
    "        severity=\"INFO\",\n",
    "        details={\n",
    "            \"ventas_negativas\": ventas_negativas,\n",
    "            \"total_ventas\": total_con_ventas,\n",
    "            \"porcentaje\": f\"{porcentaje_negativas:.2f}%\"\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Reglas Negocio - VentaNeta Negativas\",\n",
    "        \"FAIL\",\n",
    "        f\"{ventas_negativas:,} ventas negativas ({porcentaje_negativas:.2f}%) - Revisar\",\n",
    "        severity=\"WARNING\",\n",
    "        details={\n",
    "            \"ventas_negativas\": ventas_negativas,\n",
    "            \"total_ventas\": total_con_ventas,\n",
    "            \"porcentaje\": f\"{porcentaje_negativas:.2f}%\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Test 3: Verificar CantidadLitros siempre positiva cuando existe\n",
    "litros_negativos = df_final.filter(\n",
    "    F.col(\"CantidadLitros\").isNotNull() & (F.col(\"CantidadLitros\") < 0)\n",
    ").count()\n",
    "\n",
    "if litros_negativos == 0:\n",
    "    dq.log_test(\n",
    "        \"Reglas Negocio - CantidadLitros Positiva\",\n",
    "        \"PASS\",\n",
    "        \"Todas las cantidades en litros son positivas\"\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Reglas Negocio - CantidadLitros Positiva\",\n",
    "        \"FAIL\",\n",
    "        f\"{litros_negativos:,} registros con CantidadLitros negativa\",\n",
    "        details={\"litros_negativos\": litros_negativos}\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.2 Validar Combinaciones de Claves\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Validando combinaciones de claves...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: Verificar que cada combinación Detallista+Material+Fecha es única por ClaseOperacionID\n",
    "dq.check_duplicates(\n",
    "    df_final,\n",
    "    key_columns=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"],\n",
    "    threshold=0.0,\n",
    "    test_name=\"Combinaciones Claves - Unicidad\"\n",
    ")\n",
    "\n",
    "# Test 2: Verificar distribución de ClaseOperacionID\n",
    "dist_clase_operacion = (\n",
    "    df_final\n",
    "    .groupBy(\"ClaseOperacionID\")\n",
    "    .agg(F.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Distribución de ClaseOperacionID:\")\n",
    "display(dist_clase_operacion)\n",
    "\n",
    "# Test 3: Verificar que hay múltiples fechas por Detallista+Material\n",
    "combinaciones_con_multiples_fechas = (\n",
    "    df_final\n",
    "    .groupBy(\"Detallista\", \"Material\")\n",
    "    .agg(F.countDistinct(\"Fecha\").alias(\"num_fechas\"))\n",
    "    .filter(F.col(\"num_fechas\") > 1)\n",
    "    .count()\n",
    ")\n",
    "\n",
    "total_combinaciones = df_final.select(\"Detallista\", \"Material\").distinct().count()\n",
    "\n",
    "if combinaciones_con_multiples_fechas > 0:\n",
    "    dq.log_test(\n",
    "        \"Combinaciones Claves - Múltiples Fechas\",\n",
    "        \"PASS\",\n",
    "        f\"{combinaciones_con_multiples_fechas:,} combinaciones Detallista+Material tienen múltiples fechas\",\n",
    "        details={\n",
    "            \"con_multiples_fechas\": combinaciones_con_multiples_fechas,\n",
    "            \"total_combinaciones\": total_combinaciones\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dq.log_test(\n",
    "        \"Combinaciones Claves - Múltiples Fechas\",\n",
    "        \"FAIL\",\n",
    "        \"No hay combinaciones con múltiples fechas - Posible problema\",\n",
    "        severity=\"WARNING\"\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4️⃣ ANÁLISIS ESTADÍSTICO Y DETECCIÓN DE ANOMALÍAS\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.1 Estadísticas Descriptivas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"📊 Calculando estadísticas descriptivas...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obtener columnas numéricas (columnas pivotadas + VentaNeta + CantidadLitros)\n",
    "columnas_numericas = [col for col in df_final.columns if col not in [\n",
    "    \"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"\n",
    "]]\n",
    "\n",
    "# Calcular estadísticas\n",
    "df_stats = df_final.select(\n",
    "    *[\n",
    "        F.mean(col).alias(f\"{col}_mean\")\n",
    "        for col in columnas_numericas if col in df_final.columns\n",
    "    ] +\n",
    "    [\n",
    "        F.stddev(col).alias(f\"{col}_stddev\")\n",
    "        for col in columnas_numericas if col in df_final.columns\n",
    "    ] +\n",
    "    [\n",
    "        F.min(col).alias(f\"{col}_min\")\n",
    "        for col in columnas_numericas if col in df_final.columns\n",
    "    ] +\n",
    "    [\n",
    "        F.max(col).alias(f\"{col}_max\")\n",
    "        for col in columnas_numericas if col in df_final.columns\n",
    "    ] +\n",
    "    [\n",
    "        F.expr(f\"percentile({col}, 0.25)\").alias(f\"{col}_p25\")\n",
    "        for col in columnas_numericas if col in df_final.columns\n",
    "    ] +\n",
    "    [\n",
    "        F.expr(f\"percentile({col}, 0.50)\").alias(f\"{col}_median\")\n",
    "        for col in columnas_numericas if col in df_final.columns\n",
    "    ] +\n",
    "    [\n",
    "        F.expr(f\"percentile({col}, 0.75)\").alias(f\"{col}_p75\")\n",
    "        for col in columnas_numericas if col in df_final.columns\n",
    "    ]\n",
    ")\n",
    "print(\"\\n📊 Estadísticas por columna:\")\n",
    "display(df_stats)\n",
    "\n",
    "# Crear resumen más legible\n",
    "stats_dict = df_stats.first().asDict()\n",
    "for col in columnas_numericas:\n",
    "    if f\"{col}_mean\" in stats_dict:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {stats_dict.get(f'{col}_mean', 0):,.2f}\")\n",
    "        print(f\"  Std Dev: {stats_dict.get(f'{col}_stddev', 0):,.2f}\")\n",
    "        print(f\"  Min: {stats_dict.get(f'{col}_min', 0):,.2f}\")\n",
    "        print(f\"  25%: {stats_dict.get(f'{col}_p25', 0):,.2f}\")\n",
    "        print(f\"  Median: {stats_dict.get(f'{col}_median', 0):,.2f}\")\n",
    "        print(f\"  75%: {stats_dict.get(f'{col}_p75', 0):,.2f}\")\n",
    "        print(f\"  Max: {stats_dict.get(f'{col}_max', 0):,.2f}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.2 Detección de Outliers (Z-Score)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"🔍 Detectando outliers usando Z-Score...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Para cada columna numérica, calcular z-score y detectar outliers\n",
    "threshold = Config.Thresholds.OUTLIER_ZSCORE_THRESHOLD\n",
    "\n",
    "for col in [\"VentaNeta\", \"CantidadLitros\"]:\n",
    "    if col in df_final.columns:\n",
    "        # Calcular mean y stddev\n",
    "        stats = df_final.agg(\n",
    "            F.mean(col).alias(\"mean\"),\n",
    "            F.stddev(col).alias(\"stddev\")\n",
    "        ).first()\n",
    "        \n",
    "        mean_val = stats[\"mean\"]\n",
    "        stddev_val = stats[\"stddev\"]\n",
    "        \n",
    "        if stddev_val and stddev_val > 0:\n",
    "            # Calcular z-score\n",
    "            df_with_zscore = df_final.withColumn(\n",
    "                f\"{col}_zscore\",\n",
    "                F.abs((F.col(col) - mean_val) / stddev_val)\n",
    "            )\n",
    "            \n",
    "            # Contar outliers\n",
    "            outliers = df_with_zscore.filter(F.col(f\"{col}_zscore\") > threshold).count()\n",
    "            total = df_with_zscore.filter(F.col(col).isNotNull()).count()\n",
    "            outlier_percentage = (outliers / total * 100) if total > 0 else 0\n",
    "            \n",
    "            if outlier_percentage < 1:  # Menos del 1% es aceptable\n",
    "                dq.log_test(\n",
    "                    f\"Outliers - {col}\",\n",
    "                    \"PASS\",\n",
    "                    f\"{outliers:,} outliers ({outlier_percentage:.2f}%) con |z-score| > {threshold}\",\n",
    "                    severity=\"INFO\",\n",
    "                    details={\n",
    "                        \"columna\": col,\n",
    "                        \"outliers\": outliers,\n",
    "                        \"total_no_null\": total,\n",
    "                        \"porcentaje\": f\"{outlier_percentage:.2f}%\",\n",
    "                        \"threshold_zscore\": threshold\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                dq.log_test(\n",
    "                    f\"Outliers - {col}\",\n",
    "                    \"FAIL\",\n",
    "                    f\"{outliers:,} outliers ({outlier_percentage:.2f}%) - Revisar datos atípicos\",\n",
    "                    severity=\"WARNING\",\n",
    "                    details={\n",
    "                        \"columna\": col,\n",
    "                        \"outliers\": outliers,\n",
    "                        \"total_no_null\": total,\n",
    "                        \"porcentaje\": f\"{outlier_percentage:.2f}%\",\n",
    "                        \"threshold_zscore\": threshold\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            # Mostrar algunos ejemplos de outliers\n",
    "            if outliers > 0:\n",
    "                print(f\"\\n📊 Ejemplos de outliers en {col}:\")\n",
    "                display(\n",
    "                    df_with_zscore\n",
    "                    .filter(F.col(f\"{col}_zscore\") > threshold)\n",
    "                    .select(\"Detallista\", \"Material\", \"Fecha\", col, f\"{col}_zscore\")\n",
    "                    .orderBy(F.desc(f\"{col}_zscore\"))\n",
    "                    .limit(10)\n",
    "                )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.3 Análisis de Tendencias Temporales\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"📈 Analizando tendencias temporales...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Agregar por fecha para ver tendencias\n",
    "df_tendencia = (\n",
    "    df_final\n",
    "    .groupBy(\"Fecha\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_registros\"),\n",
    "        F.sum(\"VentaNeta\").alias(\"venta_total\"),\n",
    "        F.sum(\"CantidadLitros\").alias(\"litros_totales\"),\n",
    "        F.countDistinct(\"Detallista\").alias(\"detallistas_unicos\"),\n",
    "        F.countDistinct(\"Material\").alias(\"materiales_unicos\")\n",
    "    )\n",
    "    .orderBy(\"Fecha\")\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Tendencia por fecha:\")\n",
    "display(df_tendencia)\n",
    "\n",
    "# Detectar días sin datos\n",
    "todas_las_fechas = spark.sql(f\"\"\"\n",
    "    SELECT date_add('{Config.FECHA_INICIO}', pos) as Fecha\n",
    "    FROM (SELECT posexplode(split(repeat(',', datediff('{Config.FECHA_FIN}', '{Config.FECHA_INICIO}')), ',')))\n",
    "\"\"\")\n",
    "\n",
    "fechas_sin_datos = (\n",
    "    todas_las_fechas\n",
    "    .join(df_tendencia, on=\"Fecha\", how=\"left_anti\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "if fechas_sin_datos == 0:\n",
    "    dq.log_test(\n",
    "        \"Tendencias - Cobertura Diaria Completa\",\n",
    "        \"PASS\",\n",
    "        \"Hay datos para todas las fechas en el rango\"\n",
    "    )\n",
    "else:\n",
    "    total_dias = (datetime.strptime(Config.FECHA_FIN, \"%Y-%m-%d\") - \n",
    "                  datetime.strptime(Config.FECHA_INICIO, \"%Y-%m-%d\")).days + 1\n",
    "    porcentaje_sin_datos = (fechas_sin_datos / total_dias * 100)\n",
    "    \n",
    "    dq.log_test(\n",
    "        \"Tendencias - Cobertura Diaria Completa\",\n",
    "        \"FAIL\",\n",
    "        f\"{fechas_sin_datos} días sin datos ({porcentaje_sin_datos:.2f}%)\",\n",
    "        severity=\"WARNING\",\n",
    "        details={\n",
    "            \"dias_sin_datos\": fechas_sin_datos,\n",
    "            \"total_dias\": total_dias,\n",
    "            \"porcentaje\": f\"{porcentaje_sin_datos:.2f}%\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5️⃣ RESUMEN FINAL Y REPORTE\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Mostrar resumen de todos los tests\n",
    "dq.display_summary()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6️⃣ VALIDACIONES ADICIONALES RECOMENDADAS\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 6.1 Comparación con Ejecución Anterior (si existe)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"📊 Validación de regresión (si existe tabla anterior)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Si tienes una tabla con la ejecución anterior, puedes comparar\n",
    "# df_anterior = spark.table(\"damm_silver_des.analytics.rentabilidad_consolidada_anterior\")\n",
    "# \n",
    "# # Comparar conteos totales\n",
    "# count_actual = df_final.count()\n",
    "# count_anterior = df_anterior.count()\n",
    "# diferencia_porcentaje = abs(count_actual - count_anterior) / count_anterior * 100\n",
    "# \n",
    "# if diferencia_porcentaje < 10:  # Menos del 10% de cambio\n",
    "#     print(f\"✅ Conteo similar a ejecución anterior: {count_actual:,} vs {count_anterior:,}\")\n",
    "# else:\n",
    "#     print(f\"⚠️ Cambio significativo en conteo: {count_actual:,} vs {count_anterior:,} ({diferencia_porcentaje:.2f}%)\")\n",
    "\n",
    "print(\"ℹ️ Comparación con ejecución anterior no implementada (tabla anterior no disponible)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 6.2 Guardar Métricas de Calidad\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"💾 Guardando métricas de calidad...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Crear DataFrame con métricas de calidad\n",
    "metrics_data = []\n",
    "timestamp = datetime.now()\n",
    "\n",
    "# Agregar métricas clave\n",
    "metrics_data.append({\n",
    "    \"metric_name\": \"total_records\",\n",
    "    \"metric_value\": float(df_final.count()),\n",
    "    \"timestamp\": timestamp,\n",
    "    \"test_status\": \"PASS\"\n",
    "})\n",
    "\n",
    "metrics_data.append({\n",
    "    \"metric_name\": \"total_tests_run\",\n",
    "    \"metric_value\": float(dq.test_count),\n",
    "    \"timestamp\": timestamp,\n",
    "    \"test_status\": \"INFO\"\n",
    "})\n",
    "\n",
    "metrics_data.append({\n",
    "    \"metric_name\": \"tests_passed\",\n",
    "    \"metric_value\": float(dq.passed_count),\n",
    "    \"timestamp\": timestamp,\n",
    "    \"test_status\": \"PASS\"\n",
    "})\n",
    "\n",
    "metrics_data.append({\n",
    "    \"metric_name\": \"tests_failed\",\n",
    "    \"metric_value\": float(dq.failed_count),\n",
    "    \"timestamp\": timestamp,\n",
    "    \"test_status\": \"FAIL\" if dq.failed_count > 0 else \"PASS\"\n",
    "})\n",
    "\n",
    "# Crear DataFrame\n",
    "df_metrics = spark.createDataFrame(metrics_data)\n",
    "\n",
    "print(\"📊 Métricas de calidad:\")\n",
    "display(df_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_silver_pricing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
