{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9074dc94-2ca4-4fc4-a899-a5f1472d6e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Table:\n",
    "    \"\"\"Catálogo de tablas\"\"\"\n",
    "    PROMOS = 'damm_silver_des.pricing_unho.ca_crm_promos_test'\n",
    "    DESCUENTOS = 'damm_silver_des.pricing_unho.ca_crm_descuentos_test'\n",
    "    MATERIALES = 'damm_silver_des.dm.dm_material_1'\n",
    "    VENTAS = 'damm_silver_des.gest_com_horeca.ca_crm_ventas_posicion'\n",
    "    ESTABLECIMIENTOS = 'damm_silver_des.dm.dm_sf_establecimiento'\n",
    "    DETALLISTAS = 'damm_silver_des.dm.dm_sf_detallista'\n",
    "    RVL = 'damm_silver_des.gest_com_horeca.ca_crm_rentabilidad'\n",
    "\n",
    "class ClasesCondiciones:\n",
    "    \"\"\"Clasificación de condiciones comerciales\"\"\"\n",
    "    Tarifa = [\"050\"]\n",
    "    Obsequios = [\"100\"]\n",
    "    Promociones = [\"300\"]\n",
    "    Descuentos = [\"210\"]\n",
    "    Contratos = [\"400\"]\n",
    "    AmortizacionesCDT = [\"420\"]\n",
    "    Rappels = [\"740\"]\n",
    "    CostesDistribuidor = [\"900\", \"901\", \"920\", \"902\", \"930\"]\n",
    "    Costes = [\"858\", \"859\", \"861\", \"890\", \"954\", \"956\"]\n",
    "\n",
    "def create_aggregator_column(col_name: str):\n",
    "    \"\"\"\n",
    "    Crea columna AgregadorCondicion\n",
    "    \"\"\"\n",
    "    condition = None\n",
    "    for categoria, codigos in ClasesCondiciones.__dict__.items():\n",
    "        if not categoria.startswith(\"__\"):\n",
    "            if condition is None:\n",
    "                condition = F.when(F.col(col_name).isin(codigos), F.lit(categoria))\n",
    "            else:\n",
    "                condition = condition.when(F.col(col_name).isin(codigos), F.lit(categoria))\n",
    "    return condition.otherwise(F.lit(None))\n",
    "\n",
    "\n",
    "def get_materiales_filtrados(spark: SparkSession) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Obtiene y cachea materiales filtrados\n",
    "    \"\"\"\n",
    "    logger.info(\"Cargando y filtrando materiales...\")\n",
    "    \n",
    "    df = (\n",
    "        spark\n",
    "        .table(Table.MATERIALES)\n",
    "        .filter(F.col(\"Sector\") == '1')\n",
    "        .filter(~F.col(\"Material\").isin([\"TB8\", \"TB10\", \"TB12\"]))\n",
    "        .select(\"Material\")\n",
    "        .distinct()\n",
    "        .cache()  # Cache porque se usa en múltiples joins\n",
    "    )\n",
    "    \n",
    "    count = df.count()\n",
    "    logger.info(f\"Materiales filtrados: {count} registros\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rentabilidad_agregada(spark: SparkSession, df_materiales: 'DataFrame', \n",
    "                               fecha_inicio: str, fecha_fin: str) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Obtiene rentabilidad agregada por dimensiones clave\n",
    "    \"\"\"\n",
    "    logger.info(f\"Cargando rentabilidad entre {fecha_inicio} y {fecha_fin}...\")\n",
    "    \n",
    "    df = (\n",
    "        spark\n",
    "        .table(Table.RVL)\n",
    "        .filter(F.col(\"Fecha\") >= fecha_inicio)\n",
    "        .filter(F.col(\"Fecha\") <= fecha_fin)\n",
    "        .join(F.broadcast(df_materiales), on=\"Material\", how=\"inner\")\n",
    "        .withColumn(\"AgregadorCondicion\", create_aggregator_column(\"CtaResNiv2\"))\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"AgregadorCondicion\", \"claseOperacion\")\n",
    "        .agg(F.sum(\"MargenDetallado\").alias(\"MargenTotal\"))\n",
    "        .withColumnRenamed(\"claseOperacion\", \"ClaseOperacionID\")\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Rentabilidad cargada: {df.count()} registros agregados\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def ajustar_costes_distribuidor(df_rvl: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Ajusta márgenes de CostesDistribuidor restando MargenTarifa\n",
    "    \"\"\"\n",
    "    logger.info(\"Ajustando costes de distribuidor...\")\n",
    "    \n",
    "    # Paso 1: Extraer márgenes de Tarifa en una tabla separada\n",
    "    df_margen_tarifa = (\n",
    "        df_rvl\n",
    "        .filter(F.col(\"AgregadorCondicion\") == \"Tarifa\")\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "        .agg(F.sum(\"MargenTotal\").alias(\"MargenTarifa\"))\n",
    "    )\n",
    "    \n",
    "    df_ajustado = (\n",
    "        df_rvl\n",
    "        .join(\n",
    "            df_margen_tarifa,\n",
    "            on=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"],\n",
    "            how=\"left\"  # Left join para mantener todos los registros\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"MargenTotal\",\n",
    "            F.when(\n",
    "                F.col(\"AgregadorCondicion\") == \"CostesDistribuidor\",\n",
    "                F.col(\"MargenTotal\") - F.coalesce(F.col(\"MargenTarifa\"), F.lit(0))\n",
    "            ).otherwise(F.col(\"MargenTotal\"))\n",
    "        )\n",
    "        .drop(\"MargenTarifa\")\n",
    "    )\n",
    "    \n",
    "    return df_ajustado\n",
    "\n",
    "\n",
    "def get_ventas_agregadas(spark: SparkSession, df_materiales: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Obtiene ventas agregadas por dimensiones clave\n",
    "    \"\"\"\n",
    "    logger.info(\"Cargando ventas...\")\n",
    "    \n",
    "    df = (\n",
    "        spark\n",
    "        .table(Table.VENTAS)\n",
    "        .join(F.broadcast(df_materiales), on=\"Material\", how=\"inner\")\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "        .agg(\n",
    "            F.sum(\"VentaNeta\").alias(\"VentaNeta\"),\n",
    "            F.sum(\"CantidadLitros\").alias(\"CantidadLitros\")\n",
    "        )\n",
    "        .select(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\", \"CantidadLitros\", \"VentaNeta\")\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Ventas cargadas: {df.count()} registros agregados\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def pivotar_y_unir(df_rvl_ajustado: 'DataFrame', df_ventas: 'DataFrame',\n",
    "                   num_partitions: int = 200) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Pivotea rentabilidad y une con ventas\n",
    "    \"\"\"\n",
    "    logger.info(\"Pivotando rentabilidad...\")\n",
    "    \n",
    "    df_reparticionado = df_rvl_ajustado.repartition(\n",
    "        num_partitions,\n",
    "        \"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"\n",
    "    )\n",
    "    \n",
    "    df_pivotado = (\n",
    "        df_reparticionado\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "        .pivot(\"AgregadorCondicion\")\n",
    "        .agg(F.sum(\"MargenTotal\"))\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Uniendo con ventas...\")\n",
    "    \n",
    "    df_final = df_pivotado.join(\n",
    "        df_ventas,\n",
    "        on=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def main(spark: SparkSession, fecha_inicio: str = \"2024-01-01\", \n",
    "         fecha_fin: str = \"2025-12-31\", num_partitions: int = 200):\n",
    "    \"\"\"\n",
    "    Función principal del pipeline\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"Iniciando pipeline de rentabilidad optimizado\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    df_materiales = get_materiales_filtrados(spark)\n",
    "    df_rvl = get_rentabilidad_agregada(spark, df_materiales, fecha_inicio, fecha_fin)\n",
    "    df_rvl_ajustado = ajustar_costes_distribuidor(df_rvl)\n",
    "    df_ventas = get_ventas_agregadas(spark, df_materiales)\n",
    "    df_final = pivotar_y_unir(df_rvl_ajustado, df_ventas, num_partitions)\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"Pipeline completado exitosamente\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    df_materiales.unpersist()\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def pivotar_selectivo(df_rvl_ajustado: 'DataFrame', df_ventas: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Alternativa al pivot: crear solo las columnas necesarias con agregaciones\n",
    "    Útil cuando tienes muchas categorías pero solo necesitas algunas\n",
    "    \"\"\"\n",
    "    logger.info(\"Creando columnas selectivas (sin pivot)...\")\n",
    "    \n",
    "    df_agregado = (\n",
    "        df_rvl_ajustado\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "        .agg(\n",
    "            # Crear columna para cada categoría relevante\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"Tarifa\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"Tarifa\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"Obsequios\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"Obsequios\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"Promociones\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"Promociones\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"Descuentos\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"Descuentos\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"Contratos\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"Contratos\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"AmortizacionesCDT\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"AmortizacionesCDT\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"Rappels\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"Rappels\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"CostesDistribuidor\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"CostesDistribuidor\"),\n",
    "            F.sum(F.when(F.col(\"AgregadorCondicion\") == \"Costes\", F.col(\"MargenTotal\")).otherwise(0)).alias(\"Costes\")\n",
    "        )\n",
    "        .join(df_ventas, on=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"], how=\"left\")\n",
    "    )\n",
    "    \n",
    "    return df_agregado\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = (SparkSession.builder\n",
    "        .appName(\"Rentabilidad_Optimizado\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  \n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    # Ejecutar pipeline\n",
    "    df_resultado = main(spark, fecha_inicio=\"2024-01-01\", fecha_fin=\"2025-12-31\")\n",
    "    \n",
    "    # Mostrar resultado\n",
    "    df_resultado.show(10)\n",
    "    \n",
    "    # Opcionalmente guardar\n",
    "    # df_resultado.write.mode(\"overwrite\").saveAsTable(\"damm_silver_des.pricing_unho.ca_ph_pricing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21c12c8-26c6-41c0-bd88-24bd2dc05abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import logging\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Table:\n",
    "    \"\"\"Catálogo de tablas\"\"\"\n",
    "    MATERIALES = 'damm_silver_des.dm.dm_material_1'\n",
    "    VENTAS = 'damm_silver_des.gest_com_horeca.ca_crm_ventas_posicion'\n",
    "    RVL = 'damm_silver_des.gest_com_horeca.ca_crm_rentabilidad'\n",
    "    OUTPUT = 'damm_silver_des.pricing_unho.ca_ph_pricing' \n",
    "\n",
    "\n",
    "class ClasesCondiciones:\n",
    "    \"\"\"Clasificación de condiciones comerciales\"\"\"\n",
    "    Tarifa = [\"050\"]\n",
    "    Obsequios = [\"100\"]\n",
    "    Promociones = [\"300\"]\n",
    "    Descuentos = [\"210\"]\n",
    "    Contratos = [\"400\"]\n",
    "    AmortizacionesCDT = [\"420\"]\n",
    "    Rappels = [\"740\"]\n",
    "    CostesDistribuidor = [\"900\", \"901\", \"920\", \"902\", \"930\"]\n",
    "    Costes = [\"858\", \"859\", \"861\", \"890\", \"954\", \"956\"]\n",
    "\n",
    "\n",
    "class SparkConfig:\n",
    "    \"\"\"Configuración recomendada para 188M registros\"\"\"\n",
    "    \n",
    "    SHUFFLE_PARTITIONS = 400\n",
    "    AUTO_BROADCAST_THRESHOLD = 10485760\n",
    "    ADAPTIVE_ENABLED = True\n",
    "    COALESCE_PARTITIONS = True\n",
    "    SORT_MERGE_JOIN_ENABLED = True\n",
    "\n",
    "\n",
    "def create_aggregator_column(col_name: str):\n",
    "    \"\"\"\n",
    "    Crea columna AgregadorCondicion usando F.when()\n",
    "    \"\"\"\n",
    "    condition = None\n",
    "    for categoria, codigos in ClasesCondiciones.__dict__.items():\n",
    "        if not categoria.startswith(\"__\") and not categoria.startswith(\"get_\"):\n",
    "            if condition is None:\n",
    "                condition = F.when(F.col(col_name).isin(codigos), F.lit(categoria))\n",
    "            else:\n",
    "                condition = condition.when(F.col(col_name).isin(codigos), F.lit(categoria))\n",
    "    return condition.otherwise(F.lit(None))\n",
    "\n",
    "\n",
    "def configure_spark_session(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Configura Spark para manejar 188M registros eficientemente\n",
    "    \"\"\"\n",
    "    logger.info(\"Configurando Spark para grandes volúmenes...\")\n",
    "    \n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", SparkConfig.SHUFFLE_PARTITIONS)\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", SparkConfig.ADAPTIVE_ENABLED)\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", SparkConfig.COALESCE_PARTITIONS)\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", SparkConfig.AUTO_BROADCAST_THRESHOLD)\n",
    "    spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", SparkConfig.SORT_MERGE_JOIN_ENABLED)\n",
    "    \n",
    "    spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "    \n",
    "    logger.info(f\"✅ Spark configurado: {SparkConfig.SHUFFLE_PARTITIONS} shuffle partitions\")\n",
    "\n",
    "\n",
    "def get_materiales_filtrados(spark: SparkSession) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Obtiene materiales filtrados\n",
    "    \"\"\"\n",
    "    logger.info(\"Cargando materiales (225K registros)...\")\n",
    "    \n",
    "    df = (\n",
    "        spark\n",
    "        .table(Table.MATERIALES)\n",
    "        .filter(F.col(\"Sector\") == '1')\n",
    "        .filter(~F.col(\"Material\").isin([\"TB8\", \"TB10\", \"TB12\"]))\n",
    "        .select(\"Material\")\n",
    "        .distinct()\n",
    "    )\n",
    "    \n",
    "    count = df.count()\n",
    "    logger.info(f\"✅ Materiales filtrados: {count:,} registros\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rentabilidad_base(spark: SparkSession, fecha_inicio: str, fecha_fin: str) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Carga RVL con filtros optimizados\n",
    "    \"\"\"\n",
    "    logger.info(f\"Cargando RVL (188M registros) entre {fecha_inicio} y {fecha_fin}...\")\n",
    "    \n",
    "    df = (\n",
    "        spark\n",
    "        .table(Table.RVL)\n",
    "        .filter(F.col(\"Fecha\") >= fecha_inicio)\n",
    "        .filter(F.col(\"Fecha\") <= fecha_fin)\n",
    "        .select(\n",
    "            \"Detallista\",\n",
    "            \"Material\", \n",
    "            \"Fecha\",\n",
    "            \"claseOperacion\",\n",
    "            \"CtaResNiv2\",\n",
    "            \"MargenDetallado\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    count = df.count()\n",
    "    logger.info(f\"✅ RVL cargado: {count:,} registros después de filtro de fechas\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def procesar_rvl_con_materiales(df_rvl: 'DataFrame', df_materiales: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Join RVL con Materiales\n",
    "    \"\"\"\n",
    "    logger.info(\"Ejecutando join RVL + Materiales (sort-merge join)...\")\n",
    "\n",
    "    df_rvl_repart = df_rvl.repartition(SparkConfig.SHUFFLE_PARTITIONS, \"Material\")\n",
    "    df_materiales_repart = df_materiales.repartition(\"Material\")\n",
    "    \n",
    "    df_joined = (\n",
    "        df_rvl_repart\n",
    "        .join(df_materiales_repart, on=\"Material\", how=\"inner\")\n",
    "        .withColumn(\"AgregadorCondicion\", create_aggregator_column(\"CtaResNiv2\"))\n",
    "    )\n",
    "    \n",
    "    count = df_joined.count()\n",
    "    logger.info(f\"✅ Join completado: {count:,} registros\")\n",
    "    \n",
    "    return df_joined\n",
    "\n",
    "\n",
    "def agregar_rentabilidad(df: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Agrega rentabilidad por dimensiones clave\n",
    "    \"\"\"\n",
    "    logger.info(\"Agregando rentabilidad...\")\n",
    "    \n",
    "    df_repart = df.repartition(\n",
    "        SparkConfig.SHUFFLE_PARTITIONS,\n",
    "        \"Detallista\", \"Material\", \"Fecha\", \"AgregadorCondicion\", \"claseOperacion\"\n",
    "    )\n",
    "    \n",
    "    df_agregado = (\n",
    "        df_repart\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"AgregadorCondicion\", \"claseOperacion\")\n",
    "        .agg(F.sum(\"MargenDetallado\").alias(\"MargenTotal\"))\n",
    "        .withColumnRenamed(\"claseOperacion\", \"ClaseOperacionID\")\n",
    "    )\n",
    "    \n",
    "    count = df_agregado.count()\n",
    "    logger.info(f\"✅ Rentabilidad agregada: {count:,} registros\")\n",
    "    \n",
    "    return df_agregado\n",
    "\n",
    "\n",
    "def ajustar_costes_distribuidor(df_rvl: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Ajusta márgenes de CostesDistribuidor restando MargenTarifa\n",
    "    \"\"\"\n",
    "    logger.info(\"Ajustando costes de distribuidor...\")\n",
    "    \n",
    "    df_margen_tarifa = (\n",
    "        df_rvl\n",
    "        .filter(F.col(\"AgregadorCondicion\") == \"Tarifa\")\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "        .agg(F.sum(\"MargenTotal\").alias(\"MargenTarifa\"))\n",
    "    )\n",
    "\n",
    "    if df_margen_tarifa.count() < 1_000_000:\n",
    "        df_margen_tarifa = df_margen_tarifa.cache()\n",
    "        logger.info(\"✅ MargenTarifa cacheado (< 1M registros)\")\n",
    "    \n",
    "    df_ajustado = (\n",
    "        df_rvl\n",
    "        .join(\n",
    "            df_margen_tarifa,\n",
    "            on=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"MargenTotal\",\n",
    "            F.when(\n",
    "                F.col(\"AgregadorCondicion\") == \"CostesDistribuidor\",\n",
    "                F.col(\"MargenTotal\") - F.coalesce(F.col(\"MargenTarifa\"), F.lit(0))\n",
    "            ).otherwise(F.col(\"MargenTotal\"))\n",
    "        )\n",
    "        .drop(\"MargenTarifa\")\n",
    "    )\n",
    "    \n",
    "    if df_margen_tarifa.is_cached:\n",
    "        df_margen_tarifa.unpersist()\n",
    "    \n",
    "    logger.info(\"✅ Ajuste de costes completado\")\n",
    "    \n",
    "    return df_ajustado\n",
    "\n",
    "\n",
    "def get_ventas_agregadas(spark: SparkSession, df_materiales: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Obtiene ventas agregadas (22M registros)\n",
    "    \"\"\"\n",
    "    logger.info(\"Cargando y agregando ventas (22M registros)...\")\n",
    "    \n",
    "    df_ventas = spark.table(Table.VENTAS)\n",
    "    \n",
    "    df_ventas_repart = df_ventas.repartition(SparkConfig.SHUFFLE_PARTITIONS, \"Material\")\n",
    "    df_materiales_repart = df_materiales.repartition(\"Material\")\n",
    "    \n",
    "    df = (\n",
    "        df_ventas_repart\n",
    "        .join(df_materiales_repart, on=\"Material\", how=\"inner\")\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "        .agg(\n",
    "            F.sum(\"VentaNeta\").alias(\"VentaNeta\"),\n",
    "            F.sum(\"CantidadLitros\").alias(\"CantidadLitros\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    count = df.count()\n",
    "    logger.info(f\"✅ Ventas agregadas: {count:,} registros\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def pivotar_rentabilidad(df_rvl_ajustado: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Pivota rentabilidad por AgregadorCondicion\n",
    "    \"\"\"\n",
    "    logger.info(\"Pivotando rentabilidad (9 categorías)...\")\n",
    "    \n",
    "    df_repart = df_rvl_ajustado.repartition(\n",
    "        SparkConfig.SHUFFLE_PARTITIONS,\n",
    "        \"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"\n",
    "    )\n",
    "    \n",
    "    df_pivotado = (\n",
    "        df_repart\n",
    "        .groupBy(\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\")\n",
    "        .pivot(\"AgregadorCondicion\", [\n",
    "            \"Tarifa\", \"Obsequios\", \"Promociones\", \"Descuentos\", \n",
    "            \"Contratos\", \"AmortizacionesCDT\", \"Rappels\", \n",
    "            \"CostesDistribuidor\", \"Costes\"\n",
    "        ])\n",
    "        .agg(F.sum(\"MargenTotal\"))\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    count = df_pivotado.count()\n",
    "    logger.info(f\"✅ Pivot completado: {count:,} registros\")\n",
    "    \n",
    "    return df_pivotado\n",
    "\n",
    "\n",
    "def unir_con_ventas(df_pivotado: 'DataFrame', df_ventas: 'DataFrame') -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Une datos pivotados con ventas\n",
    "    NOTA: Left join porque puede haber registros sin ventas (rappels)\n",
    "    \"\"\"\n",
    "    logger.info(\"Uniendo con ventas...\")\n",
    "    \n",
    "    df_final = df_pivotado.join(\n",
    "        df_ventas,\n",
    "        on=[\"Detallista\", \"Material\", \"Fecha\", \"ClaseOperacionID\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    count = df_final.count()\n",
    "    logger.info(f\"✅ Join final completado: {count:,} registros\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def guardar_resultado(df: 'DataFrame', tabla_output: str):\n",
    "    \"\"\"\n",
    "    Guarda resultado final en tabla Delta optimizada\n",
    "    \"\"\"\n",
    "    logger.info(f\"Guardando resultado en {tabla_output}...\")\n",
    "\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"Fecha\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(tabla_output)\n",
    "    \n",
    "    logger.info(f\"✅ Resultado guardado en {tabla_output}\")\n",
    "    \n",
    "    logger.info(\"Optimizando tabla Delta...\")\n",
    "    spark.sql(f\"OPTIMIZE {tabla_output}\")\n",
    "    \n",
    "    logger.info(\"Aplicando Z-Order...\")\n",
    "    spark.sql(f\"OPTIMIZE {tabla_output} ZORDER BY (Detallista, Material)\")\n",
    "    \n",
    "    logger.info(\"✅ Optimización Delta completada\")\n",
    "\n",
    "\n",
    "def main(spark: SparkSession, \n",
    "         fecha_inicio: str = \"2024-01-01\", \n",
    "         fecha_fin: str = \"2025-12-31\",\n",
    "         tabla_output: str = None):\n",
    "    \"\"\"\n",
    "    Pipeline principal optimizado para 188M registros\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"PIPELINE RENTABILIDAD - Optimizado para 188M registros\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    configure_spark_session(spark)\n",
    "    df_materiales = get_materiales_filtrados(spark)\n",
    "    df_rvl = get_rentabilidad_base(spark, fecha_inicio, fecha_fin)\n",
    "    df_rvl_con_materiales = procesar_rvl_con_materiales(df_rvl, df_materiales)\n",
    "    df_rvl_agregado = agregar_rentabilidad(df_rvl_con_materiales)\n",
    "    df_rvl_ajustado = ajustar_costes_distribuidor(df_rvl_agregado)\n",
    "    df_ventas = get_ventas_agregadas(spark, df_materiales)\n",
    "    df_pivotado = pivotar_rentabilidad(df_rvl_ajustado)\n",
    "    df_final = unir_con_ventas(df_pivotado, df_ventas)\n",
    "    tabla_output = tabla_output or Table.OUTPUT\n",
    "    \n",
    "    guardar_resultado(df_final, tabla_output)\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"✅ PIPELINE COMPLETADO EXITOSAMENTE\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Rentabilidad_188M_Optimizado\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    df_resultado = main(\n",
    "        spark,\n",
    "        fecha_inicio=\"2024-01-01\",\n",
    "        fecha_fin=\"2025-12-31\",\n",
    "        tabla_output=\"damm_silver_des.pricing_unho.rentabilidad_consolidada\"\n",
    "    )\n",
    "    \n",
    "    logger.info(\"\\n📊 Muestra del resultado final:\")\n",
    "    df_resultado.show(10, truncate=False)\n",
    "\n",
    "    logger.info(\"\\n📊 Estadísticas del resultado:\")\n",
    "    logger.info(f\"Total registros: {df_resultado.count():,}\")\n",
    "    logger.info(f\"Particiones: {df_resultado.rdd.getNumPartitions()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pricing_analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
